{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54f72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2239\n",
      "118\n",
      "Found 2239 files belonging to 9 classes.\n",
      "Using 1792 files for training.\n",
      "Found 2239 files belonging to 9 classes.\n",
      "Using 447 files for validation.\n",
      "['actinic keratosis', 'basal cell carcinoma', 'dermatofibroma', 'melanoma', 'nevus', 'pigmented benign keratosis', 'seborrheic keratosis', 'squamous cell carcinoma', 'vascular lesion']\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 180, 180, 3)       0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 180, 180, 32)      2432      \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 180, 180, 32)      25632     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 90, 90, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 90, 90, 32)        25632     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 45, 45, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 45, 45, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 22, 22, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 22, 22, 64)        102464    \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 11, 11, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 11, 11, 64)        0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 7744)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 9)                 69705     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 277,129\n",
      "Trainable params: 277,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      " 1/56 [..............................] - ETA: 23:52 - loss: 2.1849 - accuracy: 0.0312"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    " #Defining the path for train and test images\n",
    "\n",
    "data_dir_train =pathlib.Path(r\"D:\\Bijaya\\Upgrad\\melanoma\\Train\")\n",
    "data_dir_test =pathlib.Path(r\"D:\\Bijaya\\Upgrad\\melanoma\\Test\")\n",
    "\n",
    "\n",
    "\n",
    "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
    "print(image_count_train)\n",
    "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
    "print(image_count_test)\n",
    "#Creating dataset\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "##  seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "##  make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_train,\n",
    "    seed=123,\n",
    "    validation_split= 0.2,\n",
    "    subset= 'training',\n",
    "    image_size=(img_height,img_width),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "## \n",
    "## use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
    "## resize your images to the size img_height*img_width, while writting the dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir_train,\n",
    "    seed=123,\n",
    "    validation_split= 0.2,\n",
    "    subset= 'validation',\n",
    "    image_size=(img_height,img_width),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "# List out all the classes of skin cancer and store them in a list. \n",
    "# # These correspond to the directory names in alphabetical order.\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9): \n",
    "  plt.subplot(3, 3, i + 1)\n",
    "  image = plt.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n",
    "  plt.title(class_names[i])\n",
    "  plt.imshow(image)\n",
    "###  training or validation data to visualize\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#Creating  a CNN model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "num_classes = 9\n",
    "model = Sequential([\n",
    "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
    "])\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (180, 180, 32)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "### Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# View the summary of all layers\n",
    "model.summary()\n",
    "#Training Model\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "#Visualizing training results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "#There is no improvement in accuracy but we can definitely see the overfitting problem has solved due to data augmentation\n",
    "\n",
    "#We can increase the epochs to increase the accuracy so it's too early for judgement\n",
    "\n",
    "\n",
    "#Finding on the first base model\n",
    "\n",
    "#The model is overfitting because we can also see difference in loss functions in training & test around the 10-11th epoch\n",
    "\n",
    "#The accuracy is just around 75-80% because there are enough features to remember the pattern.\n",
    "\n",
    "#But again, it's too early to comment on the overfitting & underfitting debate\n",
    "\n",
    "# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. \n",
    "data_augument = keras.Sequential([\n",
    "                             layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n",
    "                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),\n",
    "                             layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')\n",
    "])\n",
    "\n",
    "# visualize how your augmentation strategy works for one instance of training image.\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(data_augument(images)[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "   #Create the model, compile and train the model     \n",
    "        \n",
    "        ## e Dropout layer if there is an evidence of overfitting in your findings\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "num_classes = 9\n",
    "model = Sequential([ data_augument,\n",
    "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
    "      \n",
    "])\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (180, 180, 32)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "### Your code goes here\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "## train your model for 20 epochs\n",
    "epochs=20\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "#Finding from Second Model\n",
    "\n",
    "#There is no improvement in accuracy but we can definitely see the overfitting problem has solved due to data augmentation\n",
    "\n",
    "#We can increase the epochs to increase the accuracy so it's too early for judgement\n",
    "\n",
    "##\n",
    "path_list=[]\n",
    "lesion_list=[]\n",
    "for i in class_names:\n",
    "      \n",
    "    for j in data_dir_train.glob(i+'/*.jpg'):\n",
    "        path_list.append(str(j))\n",
    "        lesion_list.append(i)\n",
    "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
    "original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
    "original_df\n",
    "\n",
    "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
    "original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
    "original_df\n",
    "\n",
    "count=[]\n",
    "for i in class_names:\n",
    "    count.append(len(list(data_dir_train.glob(i+'/*.jpg'))))\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.bar(class_names,count)\n",
    "\n",
    "#- squamous cell carcinoma has least number of samples\n",
    "\n",
    "#:- actinic keratosis and dermatofibroma have proportionate number of classes. melanoma and pigmented benign keratosis have proprtionate number of classes\n",
    "\n",
    "class_names\n",
    "\n",
    "#path_to_training_dataset=\"/content/drive/MyDrive/Cancer Detection /Train\"\n",
    "import Augmentor\n",
    "for i in class_names:\n",
    "    p = Augmentor.Pipeline(r\"D:\\Bijaya\\Upgrad\\melanoma\\Train\",save_format='jpg')\n",
    "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.\n",
    "\n",
    "data_dir_train1 = pathlib.Path(r\"D:\\Bijaya\\Upgrad\\melanoma\\Train\\output\")\n",
    "image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n",
    "print(image_count_train1)\n",
    "\n",
    "##\n",
    "for i in class_names:\n",
    "      \n",
    "    for j in data_dir_train1.glob(i+'/*.jpg'):\n",
    "        path_list.append(str(j))\n",
    "        lesion_list.append(i)\n",
    "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
    "new_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
    "new_df\n",
    "\n",
    "new_df['Label'].value_counts()\n",
    "\n",
    "#Train the model on the data created using Augmentor\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "import pathlib\n",
    "\n",
    "data_dir_train1=pathlib.Path(r\"D:\\Bijaya\\Upgrad\\melanoma\\Train\")\n",
    "                             \n",
    "data_dir_train1\n",
    "image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n",
    "print(image_count_train1)\n",
    "\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "data_dir_train1=pathlib.Path(r\"D:\\Bijaya\\Upgrad\\melanoma\\Train\")\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train1,\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = \"training\",\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "#Todo: Create a validation dataset\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir_train1,\n",
    "  seed=123,\n",
    "  validation_split = 0.2,\n",
    "  subset = 'validation',\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "## use Dropout layer if there is an evidence of overfitting in your findings\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "num_classes = 9\n",
    "model = Sequential([ \n",
    "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
    "      \n",
    "])\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (180, 180, 32)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n",
    "\n",
    "# Compile your model (Choose optimizer and loss function appropriately)\n",
    "\n",
    "\n",
    "## ### Your code goes here\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs =30\n",
    "## Your code goes here, use 30 epochs.\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "#Accuracy on training data has increased by using Augmentor library\n",
    "\n",
    "#Model is still overfitting\n",
    "\n",
    "#The problem of overfitting can be solved by add more layer,neurons or adding dropout layers.\n",
    "\n",
    "#The Model can be further improved by tuning the hyperparameter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a8909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac183ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd74579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
